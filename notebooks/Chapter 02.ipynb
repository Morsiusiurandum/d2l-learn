{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "\n",
    "$ x^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:38:42.081537Z",
     "start_time": "2024-07-09T14:38:39.893652Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normal_distribution(_x, _mu, _sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi * _sigma ** 2)\n",
    "    return p * np.exp(-0.5 / _sigma ** 2 * (_x - _mu) ** 2)\n",
    "\n",
    "\n",
    "x = np.arange(-7, 7, 0.01)\n",
    "\n",
    "# 均值和标准差对\n",
    "params = [(0, 1), (0, 2), (3, 1)]\n",
    "\n",
    "for mu, sigma in params:\n",
    "    plt.plot(x, normal_distribution(x, mu, sigma), label=f'mean {mu}, standard-deviation {sigma}')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 人造数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:38:42.254445Z",
     "start_time": "2024-07-09T14:38:42.081537Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ⽣成y=Xw+b+噪声的合成数据函数\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))  # 生成均值为0，标准差为1的正态分布随机特征数据\n",
    "    y = torch.matmul(X, w) + b  # 计算标签y，y=Xw+b\n",
    "    y += torch.normal(0, 0.01, y.shape)  # 添加均值为0，标准差为0.01的噪声\n",
    "    return X, y.reshape((-1, 1))  # 返回特征数据X和标签数据y，将y reshape为列向量\n",
    "\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])  # 真实的权重向量w\n",
    "true_b = 4.2  # 真实的偏置b\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)  # 生成1000个样本的合成数据\n",
    "\n",
    "# 绘制特征(features)的第二列和对应标签(labels)的散点图\n",
    "plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1)\n",
    "# x轴标签\n",
    "plt.xlabel('Feature 2')\n",
    "# y轴标签\n",
    "plt.ylabel('Label')\n",
    "plt.title('Synthetic Data Scatter Plot')\n",
    "# 显示散点图\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:38:42.475432Z",
     "start_time": "2024-07-09T14:38:42.256453Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    \"\"\"生成随机顺序的小批量样本\"\"\"\n",
    "    num_examples = len(features)  # 获取样本数量\n",
    "    indices = list(range(num_examples))  # 创建样本索引列表\n",
    "    random.shuffle(indices)  # 将样本索引列表随机打乱，用于随机读取样本\n",
    "\n",
    "    # 生成每个batch的样本集合\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])  # 获取当前batch的样本索引\n",
    "        yield features[batch_indices], labels[batch_indices]  # 返回当前batch的特征和标签\n",
    "\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "\n",
    "def linreg(X, w, b):\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "    \"\"\"均⽅损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"⼩批量随机梯度下降\n",
    "    :param params: 要更新的参数列表\n",
    "    :param lr: 学习率\n",
    "    :param batch_size: 批量大小\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "    param.grad.zero_()\n",
    "\n",
    "\n",
    "# 设置超参数\n",
    "lr = 0.03\n",
    "num_epochs = 20\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "batch_size = 90\n",
    "\n",
    "# 外部循环遍历每个epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # 内部循环遍历数据的每个batch\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        # 计算当前batch的损失\n",
    "        l = loss(net(X, w, b), y)\n",
    "        # 求损失函数关于[w, b]的梯度\n",
    "        l.sum().backward()\n",
    "        # 使用随机梯度下降（SGD）更新参数[w, b]\n",
    "        sgd([w, b], lr, batch_size)\n",
    "\n",
    "    # 完成一个epoch后，计算整体的训练损失\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:38:43.851330Z",
     "start_time": "2024-07-09T14:38:42.475432Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造⼀个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))\n",
    "\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X), y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax回归\n",
    "\n",
    "### 分类问题\n",
    "\n",
    "假设每次输⼊是⼀个2 × 2的灰度图像。我们可以⽤⼀个标量表⽰每个像素值，\n",
    "每个图像对应四个特征$x_1, x_2, x_3, x_4$。此外，假设每个图像属于类别“猫”“鸡”和“狗”中的⼀个\n",
    "\n",
    "使用独热编码（one-hot encoding）。独热编码是⼀个向量，它的分量和类别⼀样多。\n",
    "类别对应的分量设置为1，其他所有分量设置为0。\n",
    "\n",
    "例如：标签${\\bf{y}}$将是⼀个三维向量，其中${(1,0,0)}^T$对应于“猫”、${(0,1,0)}^T$对应于“鸡”、${(0,0,1)}^T$对应于“狗”，那么输出${\\bf{y}}$就可以表示为：\n",
    "\n",
    "$$\n",
    "{\\bf{y}} \\in {\\begin{Bmatrix} {(1,0,0)}^T, & {(0,1,0)}^T, & {(0,0,1)}^T \\end{Bmatrix}}\n",
    "$$\n",
    "\n",
    "为了估计所有可能类别的条件概率，需要⼀个有多个输出的模型，每个类别对应⼀个输出。\n",
    "为了解决线性模型的分类问题，需要和输出⼀样多的仿射函数（affine function），每个输出对应于它⾃⼰的仿射函数。\n",
    "\n",
    "在上述例子中，可以组成一个方程组：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    {o_1} = {w_{11}}{x_1} + {w_{12}}{x_2} + {w_{13}}{x_3} + {w_{14}}{x_4} + {b_1} \\\\\n",
    "    {o_2} = {w_{21}}{x_1} + {w_{22}}{x_2} + {w_{23}}{x_3} + {w_{24}}{x_4} + {b_2} \\\\\n",
    "    {o_3} = {w_{31}}{x_1} + {w_{32}}{x_2} + {w_{33}}{x_3} + {w_{34}}{x_4} + {b_3} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "同样使用线性代数表示：\n",
    "\n",
    "$$\n",
    "    {\\bf{o}} = {\\bf{W}} {\\bf{x}} + {\\bf{b}}\n",
    "    \\quad \\text{其中} \\quad\n",
    "    {\\bf{b}} \\text{是一个偏置值}\n",
    "$$\n",
    "\n",
    "### softmax运算\n",
    "\n",
    "$$\n",
    "{\\widehat{\\bf{y}}} = softmax({\\bf{o}}) \n",
    "\\quad \\text{其中} \\quad\n",
    "{\\widehat{y}_i} = \\frac{exp({o_i})}{\\sum_{k=1}^{n}{exp({o_k})} }\n",
    "$$\n",
    "\n",
    "假设我们读取了⼀个批量的样本${\\bf X}$,其中样本维度为$d$,批量⼤⼩为$n$.则$\\bf X$可以表示为:\n",
    "\n",
    "$$\n",
    "{\\bf{X}} = \\begin{bmatrix} {\\bf{x_1}} & {\\bf{x_2}} & {\\cdots} & {\\bf{x_n}} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "公式就可转为更一般的形式:\n",
    "\n",
    "$$\n",
    "{\\bf{O}} = {\\bf{W}} {\\bf{X}} + {\\bf{B}} \n",
    "\\quad \\text{其中} \\quad\n",
    "{\\bf{B}} = \\underbrace{ \\begin{bmatrix} {\\bf{b}} & {\\bf{b}} & {\\cdots} & {\\bf{b}} \\end{bmatrix}}_n \\\\\\\\[2ex]\n",
    "{\\widehat{\\bf{Y}}} = softmax({\\bf{O}}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 最大似然\n",
    "softmax函数给出了⼀个估计向量${\\widehat{\\bf y}}$，我们可以将其视为“当输入样本为${\\bf x}$时, 样本属于某一类的概率分布”。\n",
    "\n",
    "例如:\n",
    "\n",
    "$$ {\\widehat{y}_1} = P( 样本为猫 | {\\bf x})$$\n",
    "\n",
    "意味着得到的向量${\\widehat{\\bf y}}$中的第一个元素$y_1$就是输入的样本${\\bf x}$属于猫的概率,\n",
    "${y_1}$越大,证明样本${\\bf x}$越有可能属于猫.\n",
    "\n",
    "假设我们的整个数据集$\\{\\bf X, \\bf Y\\}$具有$n$个样本，其中第$i$个样本由其特征向量${\\bf x}_i$和对应的标签向量${\\bf y}_i$组成\n",
    "\n",
    "那么在这种已经得知观测结果的情况下,试图对其中的参数进行估计,即是**最大似然估计**.\n",
    "\n",
    "softmax回归显然是离散型统计模型,其原始公式如下:\n",
    "\n",
    "$$\n",
    "L({\\theta}) = \\prod_{i=1}^n {P_{\\theta}(X_i = x_i)} \\quad i = 1,2,3,{\\cdots},n\n",
    "$$\n",
    "\n",
    "在例子中即为:\n",
    "\n",
    "$$\n",
    "L: \\quad \\prod_{i=1}^n {P({\\bf y}_i|{\\bf x}_i)} \\quad i = 1,2,3,{\\cdots},n\n",
    "$$\n",
    "\n",
    "求${L_{max}}$即求$ {[-log(L)]}_{min} $\n",
    "\n",
    "$$\n",
    "{-log(L)} = - \\sum_{i=1}^n log[{P({\\bf y}_i|{\\bf x}_i)}] = \\sum_{i=1}^n loss({\\bf y}_i,{\\widehat{\\bf y}}_i) \\\\\n",
    "\\text{其中}\n",
    "L: loss({\\bf y},{\\widehat{\\bf y}}) =  - \\sum_{i=1}^n {y_i} \\ln {{\\widehat y}_i}\n",
    "$$\n",
    "\n",
    "为了使损失函数得到最小值，因此需要求其关于$o_i$的梯度：\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial o_i} = \\sum_{j=1}^{n} \\dfrac{\\partial L}{\\partial {{\\widehat y}_j}} \\, \n",
    "\\dfrac{\\partial {{\\widehat y}_j}}{\\partial o_i}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\dfrac{\\partial L}{\\partial {{\\widehat y}_j}} = - \\dfrac{\\partial{}}{\\partial {{\\widehat y}_j}} ({\\sum_{i=1}^n {y_i} \\ln {{\\widehat y}_i}})\\\\[3ex]\n",
    "    \\dfrac{\\partial {{\\widehat y}_j}}{\\partial o_i} = \\dfrac{\\partial{}}{\\partial {o_i}} (\\frac{exp({o_j})}{\\sum_{k=1}^{n}{exp({o_k})} })\n",
    "\\end{cases},\n",
    "\n",
    "\\quad j = 1,2,3,{\\cdots},n\n",
    "$$\n",
    "\n",
    "则有：\n",
    "\n",
    "$$\n",
    "while \\quad i = j ,\\qquad\n",
    " \\dfrac{\\partial {{\\widehat y}_j}}{\\partial o_i} \n",
    " =  \\dfrac{\\partial {{\\widehat y}_i}}{\\partial o_i} \n",
    " = \\frac{ {exp({o_i})}{\\sum_{k=1}^{n}{exp({o_k})}} - {exp({o_i})}^2 } {{\\sum_{k=1}^{n}{exp({o_k})}}^2}\n",
    " = {{\\widehat y}_i} - {{\\widehat y}_i}^2\n",
    "\n",
    "\\\\[4ex]\n",
    "\n",
    "while \\quad i \\neq j ,\\qquad\n",
    " \\dfrac{\\partial {{\\widehat y}_j}}{\\partial o_i} \n",
    " = \\frac{ 0 - {exp({o_i})} {exp({o_j})} } {{\\sum_{k=1}^{n}{exp({o_k})}}^2}\n",
    " = - {{\\widehat y}_i} {{\\widehat y}_j}\n",
    "$$\n",
    "\n",
    "所以：\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial o_i} = \n",
    "    \\dfrac{\\partial L}{\\partial {{\\widehat y}_i}} ({{\\widehat y}_i} - {{\\widehat y}_i}^2) - (\\sum_{j=1}^{n,j \\neq i} \\dfrac{\\partial L}{\\partial {{\\widehat y}_j}} {{\\widehat y}_i} {{\\widehat y}_j})\n",
    "$$\n",
    "\n",
    "又因为：\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial {{\\widehat y}_i}} = - \\frac {y_i}{\\widehat y}_i\n",
    "$$\n",
    "\n",
    "所以：\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial o_i} \n",
    "= {y_i} ({\\widehat y}_i - 1) + {{\\widehat y}_i}\\sum_{j=1}^{n,j \\neq i}  {{y}_j}\n",
    "= {{\\widehat y}_i}\\sum_{j=1}^{n} {{y}_j} - {y_i}\n",
    "= {{\\widehat y}_i} - {y_i}\n",
    "$$\n",
    "\n",
    "Q.E.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
